{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "coefficients = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training dataset. Sample from training data: {u'is_root': False, u'text': u'ITS RAINING SIDEWAYS', u'popularity_score': 1.254698160267241, u'controversiality': 0, u'children': 0}\n",
      "Created validation dataset. Sample from validation data: {u'is_root': True, u'text': u'Kidney infection and subsequent stones. ', u'popularity_score': 0.8433369681773519, u'controversiality': 0, u'children': 0}\n",
      "Created test dataset. Sample from validation data: {u'is_root': True, u'text': u\"Some asshole shot 9 innocent people at a church.  I live in such a beautiful city, and it sucks that this made national headlines.  Charleston is beautiful.  It's been proven over and over again.  You owe it to yourself to visit.\", u'popularity_score': 0.7204745721237265, u'controversiality': 0, u'children': 1}\n",
      "Finding top words\n"
     ]
    }
   ],
   "source": [
    "# %run data_processing.py\n",
    "import numpy as np\n",
    "import json\n",
    "import string\n",
    "import operator\n",
    "from collections import *\n",
    "\n",
    "class OrderedCounter(Counter, OrderedDict):\n",
    "    pass\n",
    "\n",
    "# array is \n",
    "\n",
    "\n",
    "\"\"\"\n",
    " Brainstormed features:\n",
    " - sentiment analysis\n",
    "\"\"\"\n",
    "\n",
    "def add_features(top_words, data):\n",
    "    # Word count feature\n",
    "    # 100 data points\n",
    "    x_train = np.zeros((10000, 161))\n",
    "    y_train = np.zeros(10000)\n",
    "    i = 0\n",
    "    for data_point in data:\n",
    "        features = np.zeros(161)\n",
    "        word_list = process_string(data_point['text'])\n",
    "    \n",
    "        for word in word_list:\n",
    "            if word in top_words:\n",
    "                index = top_words.index(word)\n",
    "                features[index] += 1\n",
    "        features[160] = 1\n",
    "        x_train[i] = features\n",
    "        y_train[i] = data_point[\"popularity_score\"]\n",
    "        i = i+1\n",
    "\n",
    "    # TODO: at least two more features: these can be \n",
    "    # based on the text data, transformations of the other numeric features, or interaction terms. \n",
    "    return x_train, y_train\n",
    "\n",
    "def calculate_closed_form(X, Y):\n",
    "    coeffs = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(Y)\n",
    "    return coeffs\n",
    "\n",
    "def read_json_file():\n",
    "    file = open('./data/proj1_data.json', 'r')\n",
    "    data= file.read()\n",
    "    file.close()\n",
    "    json_data = json.loads(data)\n",
    "    return json_data\n",
    "\n",
    "def process_string(str):\n",
    "    return str.lower().split(' ')\n",
    "\n",
    "def count_top_words(data):\n",
    "    print(\"Finding top words\")\n",
    "    word_freq = OrderedCounter()\n",
    "    for line in data:\n",
    "        word_list = process_string(line['text'])\n",
    "        word_freq.update(word_list)\n",
    "    most_common = dict(word_freq.most_common(160))\n",
    "    return list(most_common.keys())\n",
    "\n",
    "def split_data(data, first_split, second_split, third_split):\n",
    "    train, validation, test = [], [], []\n",
    "\n",
    "    for i in range(0, first_split):\n",
    "        train.append(data[i])\n",
    "    print((\"Created training dataset. Sample from training data: {}\").format(train[0]))\n",
    "\n",
    "    for i in range(first_split, second_split):\n",
    "        validation.append(data[i])\n",
    "    print((\"Created validation dataset. Sample from validation data: {}\").format(validation[0]))\n",
    "\n",
    "    for i in range(second_split, third_split):\n",
    "        test.append(data[i]) \n",
    "    print((\"Created test dataset. Sample from validation data: {}\").format(test[0]))\n",
    "\n",
    "    return train, validation, test\n",
    "\n",
    "def main():\n",
    "    data = read_json_file()\n",
    "\n",
    "    \"\"\"\n",
    "    Use the first 10,000 points for training, the next 1,000 for validation, and the final 1,000 for testing.\n",
    "    \"\"\"\n",
    "    train, validation, test = split_data(data, 10000, 11000, 12000)\n",
    "\n",
    "    top_words = count_top_words(train)\n",
    "    # print(top_words)\n",
    "    \n",
    "    data_with_features, y_train = add_features(top_words, train)\n",
    "    coefficients = calculate_closed_form(data_with_features, y_train)\n",
    "    return coefficients\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    coefficients = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.11812793e-02, -1.97377137e-02, -4.45503046e-03, -4.29237766e-02,\n",
       "       -2.36255504e-02,  9.98394724e-02, -7.82411651e-02, -3.87072263e-01,\n",
       "       -3.38520978e-02,  6.30789860e-02,  1.09819629e-01, -6.17252928e-02,\n",
       "        4.17305065e-02, -9.08702703e-03,  2.59126027e-02, -1.80077060e-02,\n",
       "        4.55601484e-02,  1.64503088e-01, -2.75914251e-02,  8.87745571e-02,\n",
       "        1.58733039e-01, -1.33735780e-01,  7.37559241e-04, -8.14110232e-02,\n",
       "       -2.17491648e-02,  3.00791566e-02,  4.15889554e-02,  1.32264336e-01,\n",
       "        8.04462890e-02, -1.44723802e-02,  2.34401332e-02,  3.05711321e-02,\n",
       "        2.82310895e-02,  6.57065798e-03, -2.05093882e-02, -4.43061696e-02,\n",
       "       -1.54628443e-02, -8.29632438e-03,  8.96239145e-03,  2.23969588e-02,\n",
       "        2.78214295e-02,  1.14427431e-01, -4.77994927e-02, -8.25836886e-04,\n",
       "        1.79550540e-01, -1.03604442e-01,  3.11977638e-02,  2.03928484e-02,\n",
       "       -4.95551502e-02,  3.03552320e-02,  4.59889340e-02, -6.44799031e-03,\n",
       "       -8.21024874e-02,  5.72865194e-02,  1.87013707e-02, -2.31285439e-01,\n",
       "       -2.51241664e-01,  3.37554992e-02,  1.25258636e-01, -7.18602611e-02,\n",
       "        2.61712558e-02,  3.96949147e-03, -1.92987547e-02, -1.53584456e-01,\n",
       "       -9.27488972e-02,  1.04755075e-01, -1.58624730e-01, -7.67603930e-02,\n",
       "        2.55410983e-02,  5.46696856e-02,  6.49853702e-02,  1.13591174e-02,\n",
       "        7.84352059e-02,  4.28513387e-02, -1.68562610e-01,  4.01696675e-01,\n",
       "       -7.63761236e-02,  6.87893748e-02, -8.10433194e-03,  8.37071778e-02,\n",
       "        8.09933721e-02,  1.23831444e-01, -3.35332603e-02, -6.37286105e-03,\n",
       "       -8.49747696e-02,  1.04659209e-01, -3.42595204e-01, -9.42308997e-03,\n",
       "        7.76184329e-02,  6.27050588e-02, -1.00563673e-01, -1.22592094e-01,\n",
       "       -1.31783679e-02, -3.85479592e-02, -7.73521856e-02, -6.99487665e-05,\n",
       "        4.47553369e-02, -4.15756359e-02,  1.90827876e-04,  2.88018294e-02,\n",
       "        3.32189426e-02, -2.19567160e-01,  1.97048468e-02,  4.92699076e-02,\n",
       "        2.19529401e-02, -4.68245817e-02, -1.38565863e-03,  4.31279020e-02,\n",
       "        3.77256118e-03, -3.93909377e-02,  6.62283346e-02,  7.45491143e-03,\n",
       "        5.79075938e-02,  5.00303669e-02, -1.88313100e-02, -7.28899143e-02,\n",
       "       -1.37250844e-02, -5.12407328e-02,  6.10643689e-02,  1.22486396e-01,\n",
       "       -5.98763523e-04, -1.01596436e-02,  9.75037742e-02,  2.04918854e-03,\n",
       "       -3.01356732e-02,  4.33746214e-02,  1.70676143e-02,  8.94213644e-04,\n",
       "       -3.68957644e-02,  1.59387582e-01,  2.70543741e-02, -3.27289408e-02,\n",
       "        2.84786506e-02, -8.53787199e-02,  3.26737859e-03, -4.29708242e-02,\n",
       "       -8.23136472e-02, -2.25829066e-02,  2.55760125e-02,  8.55529169e-02,\n",
       "        2.46032895e-02,  1.59203460e-02,  1.47199086e-02,  3.44994087e-02,\n",
       "       -1.06533266e-01, -1.73086387e-01, -4.55104932e-02, -2.62867355e-02,\n",
       "        7.42963975e-02,  4.27992078e-02, -1.00963057e-01, -2.81227668e-02,\n",
       "       -1.34574880e-02,  6.30224455e-02, -1.71452172e-02, -5.98137558e-02,\n",
       "        2.76286238e-02, -6.19602115e-02, -1.57427426e-02, -1.35262436e-02,\n",
       "        8.79726720e-01])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
